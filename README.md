![header](https://capsule-render.vercel.app/api?type=transparent&color=white&height=300&section=header&text=Humane&animation=blinking&fontSize=90&fontColor=d6ace6)



# GPT-reasoning-prompting
:octocat: Repository for data and code that tests the reasoning ability of the GPT model.

## I USE
<img src="https://img.shields.io/badge/python-3776AB?style=flat-square&logo=Python&logoColor=white"/> <img src="https://img.shields.io/badge/pytorch-EE4C2C?style=flat-square&logo=Pytorch&logoColor=white"/>

## It is started by
How Language Model Hallucinations Can Snowball : https://arxiv.org/abs/2305.13534

In my opinion, I thought that the dataset in this paper would be biased because it is a dataset with only yes or no.

## Dataset

asdf

## Expriment

asdf 

## Result

|제목|내용|설명|
|------|---|---|
|테스트1|테스트2|테스트3|
|테스트1|테스트2|테스트3|
|테스트1|테스트2|테스트3|
